{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from utils.exp_saver import ExpSaver\n",
    "\n",
    "from data_loading.load_cas import load_cas\n",
    "from data_loading.load_embeddings import load_vectors\n",
    "from data_loading.load_neg_filters import load_filters\n",
    "\n",
    "from data_processing.preprocessing import preprocess_data, map_text\n",
    "\n",
    "from model.LSTM import embed_bilstm\n",
    "\n",
    "from train.train import train\n",
    "from utils.utils import format_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from gensim.models.fasttext import load_facebook_model, load_facebook_vectors\n",
    "\n",
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vec = load_facebook_vectors('../embeddings/cc.fr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_predict(\n",
    "    text,\n",
    "    model_path,\n",
    "    params,\n",
    "    embeddings=None,\n",
    "    ):\n",
    "    \n",
    "    n_fold = params['n_fold']\n",
    "    epochs = params['epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    cb_verbose = False\n",
    "    hyper_params = params['hyper_params']\n",
    "    model_structure = params['model_structure']\n",
    "\n",
    "    scores = []\n",
    "    models = []\n",
    "\n",
    "    raw_sentences = map_text(\n",
    "        text,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "\n",
    "    sentences = preprocess_data(\n",
    "        raw_sentences,\n",
    "        model_structure=model_structure,\n",
    "        pad_sentences=(embeddings is None),\n",
    "        return_sentences_only=True,\n",
    "    )\n",
    "    \n",
    "    # see if possible to do it here or best to do it with each CV split\n",
    "    max_sentence_size = sentences.shape[1]\n",
    "    input_size = sentences.shape[-1] \n",
    "\n",
    "    sentences = np.squeeze(sentences)\n",
    "    \n",
    "    print(\"\"\"\n",
    "    ########################\n",
    "    # Beginning prediction #\n",
    "    ########################\n",
    "    \"\"\")\n",
    "    \n",
    "    # Load model function\n",
    "    with open(os.path.join(model_path, 'model_function.pkl'), 'rb') as f:\n",
    "        model_function = pkl.load(f)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = model_function(\n",
    "        input_size=input_size,\n",
    "        sentence_size=max_sentence_size,\n",
    "        output_size=3,\n",
    "        device=device,\n",
    "        hyper_params=hyper_params,\n",
    "    ).cuda(device)\n",
    "    \n",
    "    # Load model weights\n",
    "    state_dict = torch.load(os.path.join(model_path, 'model.h5'), map_location=device)\n",
    "                            \n",
    "    module_names = [n[0] for n in model.named_modules()]\n",
    "    existing_state_dict = {k: v for k, v in state_dict.items() if k.split('.')[0] in module_names}\n",
    "    \n",
    "    model.load_state_dict(existing_state_dict)\n",
    "\n",
    "    print(model.name)\n",
    "    # print(model)\n",
    "\n",
    "    results = model.predict(\n",
    "        sentences,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    id2tag = {0: 'O', 1: 'B-NEG', 2: 'I-NEG'}\n",
    "\n",
    "    labels = torch.max(results, -1)[1].cpu().numpy()\n",
    "    labels = np.vectorize(id2tag.get)(labels)\n",
    "    \n",
    "    annot = []\n",
    "    \n",
    "    for i, sentence in enumerate(text):\n",
    "        ann = []\n",
    "        for sent, lab in zip(sentence, labels[i]):\n",
    "            ann.append([sent, lab])\n",
    "        annot.append(ann)\n",
    "        \n",
    "    return np.array(annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ########################\n",
      "    # Beginning prediction #\n",
      "    ########################\n",
      "    \n",
      "torch_BiLSTM\n"
     ]
    }
   ],
   "source": [
    "import ruamel.yaml\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "\n",
    "def predict(\n",
    "    data_path,\n",
    "    model_path,\n",
    "    embeddings_path=None,\n",
    "    extensions=None,\n",
    "    ft_vec=None,\n",
    "):\n",
    "\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    \n",
    "    if os.path.isfile(data_path):\n",
    "        with open(data_path, 'r') as f:\n",
    "            text = f.read().split('.')\n",
    "        text = np.array([np.array([n for n in nlp(t)]) for t in text])\n",
    "        \n",
    "    elif os.path.isdir(data_path):\n",
    "        assert extensions is not None, \"data_path is a directory but no extensions is defined\"\n",
    "        # consider all the files in the directory\n",
    "        all_files = glob.glob(os.path.join(data_path, '|'.join([f'*.{ext}' for ext in extensions])))\n",
    "        text = [nlp(open(f).read()) for f in tqdm(all_files)]\n",
    "    else:\n",
    "        \"This should probably not happen but data_path is neither a file nor a directory\"\n",
    "\n",
    "    # Load params\n",
    "    with open(os.path.join(model_path, 'config.yaml'), 'r') as f:\n",
    "        params = ruamel.yaml.safe_load(f)\n",
    "        \n",
    "    if params.get(\"pretrainedEmbed\") and not embeddings_path:\n",
    "        print('Embeddings specified in model parameters but no embedding path specified, switching to no embeddings')\n",
    "        params['pretrainedEmbed'] = False\n",
    "        \n",
    "#     if params['pretrainedEmbed']=='fasttext':\n",
    "#         ft_vec = load_facebook_vectors(embeddings_path)\n",
    "#     else:\n",
    "#         ft_vec = None\n",
    "    \n",
    "    annot = torch_predict(\n",
    "        text,\n",
    "        model_path,\n",
    "        params,\n",
    "        embeddings=ft_vec,\n",
    "        )\n",
    "    \n",
    "    # load params from config.yaml in model_path\n",
    "    # load model from model.h5 in model_path\n",
    "    \n",
    "    return annot\n",
    "    \n",
    "annotation = predict(\n",
    "    data_path='/home/taille/these/data/corpus_xavier/00001-W-44--1006755177289277516--873794080268159383-8548032564789574962.txt',\n",
    "    model_path='/home/taille/these/logs/torch_BiLSTM___pretrainedEmbed+fasttext___label_level+word___neg_only+False___n_fold+10___batch_size+128___epochs+30___lr+0.01___lr_decay+1e-06___no_pos___no_cue/16012020_124030',\n",
    "    embeddings_path='../embeddings/cc.fr.300.bin',\n",
    "    extensions=['txt'],\n",
    "    ft_vec=ft_vec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions\n",
    "\n",
    "result_file = '/home/taille/these/predictions/results.conll'\n",
    "\n",
    "with open(result_file, 'w') as f:\n",
    "    for annot in annotation:\n",
    "        for word, label in annot:\n",
    "            if str(word).strip(' \\n')!='':\n",
    "                f.write(str(word) + ' ' + label + '\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
