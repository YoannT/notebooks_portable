{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import local modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from utils.exp_saver import ExpSaver\n",
    "\n",
    "from data_loading.load_cas import load_cas\n",
    "from data_loading.load_embeddings import load_vectors\n",
    "from data_loading.load_neg_filters import load_filters\n",
    "\n",
    "from data_processing.preprocessing import preprocess_data, add_tags_per_word\n",
    "\n",
    "from model.LSTM import embed_bilstm\n",
    "\n",
    "from train.train import train\n",
    "from utils.utils import format_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import universal modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5MKdgp8uXVf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from gensim.models.fasttext import load_facebook_model, load_facebook_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taille/.conda/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UnsafeLoaderWarning: \n",
      "The default 'Loader' for 'load(stream)' without further arguments can be unsafe.\n",
      "Use 'load(stream, Loader=ruamel.yaml.Loader)' explicitly if that is OK.\n",
      "Alternatively include the following in your code:\n",
      "\n",
      "  import warnings\n",
      "  warnings.simplefilter('ignore', ruamel.yaml.error.UnsafeLoaderWarning)\n",
      "\n",
      "In most other cases you should consider using 'safe_load(stream)'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pretrainedEmbed': 'fasttext',\n",
       " 'label_level': 'sentence',\n",
       " 'neg_only': False,\n",
       " 'n_fold': 10,\n",
       " 'batch_size': 32,\n",
       " 'epochs': 100,\n",
       " 'hyper_params': {'lr': 0.001, 'lr_decay': 0, 'dropout': 0.3},\n",
       " 'model_structure': {'with_attention': True,\n",
       "  'lib': 'torch',\n",
       "  'with_crf': False,\n",
       "  'word_embed': 300,\n",
       "  'pos_embed': None,\n",
       "  'cue_embed': None,\n",
       "  'lstm_size': 300}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ruamel.yaml\n",
    "with open('params.yml') as f:\n",
    "    params = ruamel.yaml.load(f)\n",
    "    \n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5eq3-EqwR6Hi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 412/75258 [00:00<00:18, 4115.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping negation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75258/75258 [00:47<00:00, 1579.80it/s]\n"
     ]
    }
   ],
   "source": [
    "cas = load_cas()\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "# get ids of sentences containing negation\n",
    "negative_sentence_ids = cas.groupby('sentence_id').apply(lambda x : x['label'].str.contains('neg')).dropna().index.get_level_values(0).unique()\n",
    "\n",
    "classes = ['O_neg', 'B_neg', 'I_neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retain only neg or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_only = load_filters() if params['neg_only']=='filter' else params['neg_only']\n",
    "\n",
    "if isinstance(neg_only, str):\n",
    "    df1 = add_tags_per_word(cas, neg_only)  \n",
    "    merge = cas.reset_index().merge(df1, left_index=True, right_index=True)\n",
    "    neg_sentences = merge[merge['label_y'].str.contains('B_neg|I_neg')]['sentence_id_x'].unique()\n",
    "    cas_neg = cas[cas['sentence_id'].isin(neg_sentences)]\n",
    "elif neg_only==True:\n",
    "    cas_neg = cas.groupby('sentence_id').filter(lambda x: (x['label'].str.contains('B').any()) or (x['label'].str.contains('I').any()))\n",
    "else:\n",
    "    cas_neg = cas.copy()\n",
    "        \n",
    "if neg_only:\n",
    "    reset_sentence_dic = {w: i for i,w in enumerate(cas_neg['sentence_id'].unique())}\n",
    "    cas_neg['sentence_id'] = cas_neg['sentence_id'].map(reset_sentence_dic).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check filter accuracy, sentence wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(neg_only, bool):\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    true_neg = []\n",
    "    pred_neg = []\n",
    "\n",
    "    for sentence_id, sentence in cas.groupby('sentence_id'):\n",
    "\n",
    "        true_labels = sentence['label'].unique()\n",
    "\n",
    "        true_neg.append(('B_scope_neg' in true_labels) or ('I_scope_neg' in true_labels))\n",
    "\n",
    "        pred_neg.append(sentence_id in neg_sentences)\n",
    "\n",
    "        if (sentence_id in neg_sentences) and not (('B_scope_neg' in true_labels) or ('I_scope_neg' in true_labels)):\n",
    "            display(sentence)\n",
    "\n",
    "\n",
    "    print(\"Predicted support: \\nO_neg: %d\\nI_neg: %d\"%(len(pred_neg)-sum(pred_neg), sum(pred_neg)))\n",
    "    print(classification_report(true_neg, pred_neg, target_names=['O_neg', 'I_neg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT PUNCTUATION (with punctuation, results crumble)\n",
    "\n",
    "# Predicted support: \n",
    "# O_neg: 2888\n",
    "# I_neg: 902\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        O_neg       0.99      0.96      0.98      2984\n",
    "#        I_neg       0.87      0.97      0.92       806\n",
    "\n",
    "#     accuracy                           0.96      3790\n",
    "#    macro avg       0.93      0.97      0.95      3790\n",
    "# weighted avg       0.97      0.96      0.96      3790"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample one random negative sentence to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>lem</th>\n",
       "      <th>postag</th>\n",
       "      <th>cue_tag</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79305</th>\n",
       "      <td>3464</td>\n",
       "      <td>0</td>\n",
       "      <td>Un</td>\n",
       "      <td>un</td>\n",
       "      <td>DET:ART</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79306</th>\n",
       "      <td>3464</td>\n",
       "      <td>1</td>\n",
       "      <td>homme</td>\n",
       "      <td>homme</td>\n",
       "      <td>NOM</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79307</th>\n",
       "      <td>3464</td>\n",
       "      <td>2</td>\n",
       "      <td>âgé</td>\n",
       "      <td>âgé</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79308</th>\n",
       "      <td>3464</td>\n",
       "      <td>3</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>PRP</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79309</th>\n",
       "      <td>3464</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>card</td>\n",
       "      <td>NUM</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79310</th>\n",
       "      <td>3464</td>\n",
       "      <td>5</td>\n",
       "      <td>ans</td>\n",
       "      <td>an</td>\n",
       "      <td>NOM</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79312</th>\n",
       "      <td>3464</td>\n",
       "      <td>7</td>\n",
       "      <td>sans</td>\n",
       "      <td>sans</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B_cue_neg</td>\n",
       "      <td>B_scope_neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79313</th>\n",
       "      <td>3464</td>\n",
       "      <td>8</td>\n",
       "      <td>antécédents</td>\n",
       "      <td>antécédent</td>\n",
       "      <td>NOM</td>\n",
       "      <td>_</td>\n",
       "      <td>I_scope_neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79314</th>\n",
       "      <td>3464</td>\n",
       "      <td>9</td>\n",
       "      <td>pathologiques</td>\n",
       "      <td>pathologique</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>_</td>\n",
       "      <td>I_scope_neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79315</th>\n",
       "      <td>3464</td>\n",
       "      <td>10</td>\n",
       "      <td>particuliers</td>\n",
       "      <td>particulier</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>_</td>\n",
       "      <td>I_scope_neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79317</th>\n",
       "      <td>3464</td>\n",
       "      <td>12</td>\n",
       "      <td>se</td>\n",
       "      <td>se</td>\n",
       "      <td>PRO:PER</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79318</th>\n",
       "      <td>3464</td>\n",
       "      <td>13</td>\n",
       "      <td>plaignait</td>\n",
       "      <td>plaindre</td>\n",
       "      <td>VER:impf</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79319</th>\n",
       "      <td>3464</td>\n",
       "      <td>14</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>PRP</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79320</th>\n",
       "      <td>3464</td>\n",
       "      <td>15</td>\n",
       "      <td>douleurs</td>\n",
       "      <td>douleur</td>\n",
       "      <td>NOM</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79321</th>\n",
       "      <td>3464</td>\n",
       "      <td>16</td>\n",
       "      <td>abdominales</td>\n",
       "      <td>abdominal</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79322</th>\n",
       "      <td>3464</td>\n",
       "      <td>17</td>\n",
       "      <td>depuis</td>\n",
       "      <td>depuis</td>\n",
       "      <td>PRP</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79323</th>\n",
       "      <td>3464</td>\n",
       "      <td>18</td>\n",
       "      <td>deux</td>\n",
       "      <td>deux</td>\n",
       "      <td>NUM</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79324</th>\n",
       "      <td>3464</td>\n",
       "      <td>19</td>\n",
       "      <td>ans</td>\n",
       "      <td>an</td>\n",
       "      <td>NOM</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79326</th>\n",
       "      <td>3464</td>\n",
       "      <td>21</td>\n",
       "      <td>initialement</td>\n",
       "      <td>initialement</td>\n",
       "      <td>ADV</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79327</th>\n",
       "      <td>3464</td>\n",
       "      <td>22</td>\n",
       "      <td>soulagées</td>\n",
       "      <td>soulager</td>\n",
       "      <td>VER:pper</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79328</th>\n",
       "      <td>3464</td>\n",
       "      <td>23</td>\n",
       "      <td>par</td>\n",
       "      <td>par</td>\n",
       "      <td>PRP</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79329</th>\n",
       "      <td>3464</td>\n",
       "      <td>24</td>\n",
       "      <td>un</td>\n",
       "      <td>un</td>\n",
       "      <td>DET:ART</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79330</th>\n",
       "      <td>3464</td>\n",
       "      <td>25</td>\n",
       "      <td>traitement</td>\n",
       "      <td>traitement</td>\n",
       "      <td>NOM</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79331</th>\n",
       "      <td>3464</td>\n",
       "      <td>26</td>\n",
       "      <td>symptomatique</td>\n",
       "      <td>symptomatique</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id  word_id           word            lem    postag  \\\n",
       "79305         3464        0             Un             un   DET:ART   \n",
       "79306         3464        1          homme          homme       NOM   \n",
       "79307         3464        2            âgé            âgé       ADJ   \n",
       "79308         3464        3             de             de       PRP   \n",
       "79309         3464        4             41           card       NUM   \n",
       "79310         3464        5            ans             an       NOM   \n",
       "79312         3464        7           sans           sans       PRP   \n",
       "79313         3464        8    antécédents     antécédent       NOM   \n",
       "79314         3464        9  pathologiques   pathologique       ADJ   \n",
       "79315         3464       10   particuliers    particulier       ADJ   \n",
       "79317         3464       12             se             se   PRO:PER   \n",
       "79318         3464       13      plaignait       plaindre  VER:impf   \n",
       "79319         3464       14             de             de       PRP   \n",
       "79320         3464       15       douleurs        douleur       NOM   \n",
       "79321         3464       16    abdominales      abdominal       ADJ   \n",
       "79322         3464       17         depuis         depuis       PRP   \n",
       "79323         3464       18           deux           deux       NUM   \n",
       "79324         3464       19            ans             an       NOM   \n",
       "79326         3464       21   initialement   initialement       ADV   \n",
       "79327         3464       22      soulagées       soulager  VER:pper   \n",
       "79328         3464       23            par            par       PRP   \n",
       "79329         3464       24             un             un   DET:ART   \n",
       "79330         3464       25     traitement     traitement       NOM   \n",
       "79331         3464       26  symptomatique  symptomatique       ADJ   \n",
       "\n",
       "         cue_tag        label  \n",
       "79305          _            _  \n",
       "79306          _            _  \n",
       "79307          _            _  \n",
       "79308          _            _  \n",
       "79309          _            _  \n",
       "79310          _            _  \n",
       "79312  B_cue_neg  B_scope_neg  \n",
       "79313          _  I_scope_neg  \n",
       "79314          _  I_scope_neg  \n",
       "79315          _  I_scope_neg  \n",
       "79317          _            _  \n",
       "79318          _            _  \n",
       "79319          _            _  \n",
       "79320          _            _  \n",
       "79321          _            _  \n",
       "79322          _            _  \n",
       "79323          _            _  \n",
       "79324          _            _  \n",
       "79326          _            _  \n",
       "79327          _            _  \n",
       "79328          _            _  \n",
       "79329          _            _  \n",
       "79330          _            _  \n",
       "79331          _            _  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "random_neg_sentence = np.random.choice(negative_sentence_ids)\n",
    "display(cas[cas['sentence_id']==random_neg_sentence])\n",
    "\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['pretrainedEmbed']==True:\n",
    "    ft_vec = load_vectors()\n",
    "\n",
    "    ft_set = set(ft_vec.keys())\n",
    "    cas_set = set(cas_neg['lem'].unique())\n",
    "\n",
    "    inter_set = set(ft_set & cas_set)\n",
    "\n",
    "    print(\"Fast text vocab: %d \\nCAS vocab: %d \\nIntersection vocab: %d\"%(len(ft_set), len(cas_set), len(inter_set)))\n",
    "elif params['pretrainedEmbed']=='bin':\n",
    "    ft_vec = load_facebook_vectors('../embeddings/cc.fr.300.bin')\n",
    "    # use ft_vec.wv['word'] to get vector values, works with long expressions too (\"cancer du sein\")\n",
    "else:\n",
    "    ft_vec = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVprGDILRKdW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function attention_bilstm at 0x2aaaeb2f7ae8>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer bidirectional_1: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-359c290f11f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcas_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mft_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/these/src/train/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cas_neg, params, embeddings, train_size)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mwith_crf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_structure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'with_crf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mhyper_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mmodel_structure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         )\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/these/src/model/LSTM.py\u001b[0m in \u001b[0;36mattention_bilstm\u001b[0;34m(sentence_size, vocab_size, postag_size, cue_size, label_size, with_crf, hyper_params, model_structure, embedding_weights, return_embeddings)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     bilstm = Bidirectional(LSTM(model_structure['lstm_size'], return_sequences=True, \n\u001b[0;32m--> 156\u001b[0;31m recurrent_dropout=0.1))(concat)\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# attention = AttentionDecoder(model_structure['lstm_size'], sentence_size)(bilstm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer bidirectional_1: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import gc\n",
    "\n",
    "with Pool(1):\n",
    "    gc.collect()\n",
    "    result_dict = train(\n",
    "        cas_neg,\n",
    "        params,\n",
    "        embeddings=ft_vec,\n",
    "    )\n",
    "\n",
    "    model = result_dict['models']\n",
    "    model_function = result_dict['model_function']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# IMPORTANT SPECIAL CASE:\n",
    "# -APOSTROPHES (do not remove, attach to first character)\n",
    "\n",
    "def attach_apostrophe(\n",
    "    sentences,\n",
    "):\n",
    "    res    \n",
    "    \n",
    "    return res\n",
    "    \n",
    "def preprocess_sentences(\n",
    "    sentences,\n",
    "    embeddings,\n",
    "    max_sentence_size=99, # should match model shape\n",
    "    ):\n",
    "    \n",
    "    raw_sentences = np.zeros((len(sentences), max_sentence_size, embeddings.wv.vector_size))\n",
    "    split_sentences = []\n",
    "    \n",
    "    for idx, sent in enumerate(sentences):\n",
    "        sentence = re.split('\\W+', sent)\n",
    "        split_sentences.append(sentence)\n",
    "        raw_sentences[idx, :len(sentence)] = np.array([embeddings.wv[w] for w in sentence])\n",
    "        \n",
    "    return split_sentences, raw_sentences\n",
    "\n",
    "sentences = [\n",
    "    \"Il n'a pas suivi de régime pendant un an.\",\n",
    "    \"Il est suivi par un médecin et ne consulte qu'au besoin.\",\n",
    "    \"Il n'a pas de bonne grammaire et de bon orthograf.\",\n",
    "    \"Bonjour j'ai un cancer mais je n'ai pas soif.\",\n",
    "    \"Bonjour je n'ai pas de cancer mais j'ai soif.\",\n",
    "    \"Le patient n'a jamais eu d'embolie pulmonaire ni de saignements.\",\n",
    "    \"Le patient n'a jamais eu d'embolie pulmonaire et de saignements.\",\n",
    "    \"Le patient n'a jamais eu d'embolie pulmonaire mais des saignements.\",\n",
    "    \"Nous n'avons pas injecté 100mg de lorazépam.\",\n",
    "    \"Le lupus n'apparaît pas sur l'IRM.\",\n",
    "    \"Aucune tumeur n'est présente dans les scans de l'oncle du patient.\",\n",
    "]\n",
    "\n",
    "split_sentences, embed_sentences = preprocess_sentences(\n",
    "    sentences,\n",
    "    ft_vec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    embed_sentences,\n",
    "    model,\n",
    "):\n",
    "\n",
    "    predict_dict = {\n",
    "        'word_input': embed_sentences,\n",
    "    }\n",
    "\n",
    "    classes = {\n",
    "        0: 'O_neg',\n",
    "        1: 'B_neg',\n",
    "        2: 'I_neg',\n",
    "    }\n",
    "\n",
    "    y_prob = model.predict(predict_dict)\n",
    "    y_classes = y_prob.argmax(axis=-1)\n",
    "    results = np.vectorize(classes.get)(y_classes)\n",
    "    \n",
    "    labeled_sentences = {\n",
    "        'sentence_id': [],\n",
    "        'word': [],\n",
    "        'label': [],\n",
    "    }\n",
    "\n",
    "    for sentence_id, (sentence, labels) in enumerate(zip(split_sentences, results)):\n",
    "        for word, label in zip(sentence, labels):\n",
    "            labeled_sentences['sentence_id'].append(sentence_id)\n",
    "            labeled_sentences['word'].append(word)\n",
    "            labeled_sentences['label'].append(label)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(labeled_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_sentences = predict(\n",
    "    embed_sentences,\n",
    "    model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, sentence in labeled_sentences.groupby('sentence_id'):\n",
    "    display(sentence.transpose())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "corpus_dalloux.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
